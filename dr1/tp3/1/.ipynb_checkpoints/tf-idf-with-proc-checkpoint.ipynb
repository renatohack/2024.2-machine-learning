{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crMUqFj2ZJOK"
   },
   "source": [
    "### Inicialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0lKI852Ix9g",
    "outputId": "0a53cdce-0af9-4325-d7a9-e565186ccf1e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.5.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "nltk.download('rslp')\n",
    "\n",
    "!pip install Unidecode\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from nltk import tokenize\n",
    "from string import punctuation\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vHnTC4OeLpkW",
    "outputId": "d3ec0f2d-39fb-4bee-eabc-3ec3255d521d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>If you've ever been to Disneyland anywhere you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Its been a while since d last time we visit HK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Thanks God it wasn   t too hot or too humid wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HK Disneyland is a great compact park. Unfortu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the location is not in the city, took around 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       4  If you've ever been to Disneyland anywhere you...\n",
       "1       4  Its been a while since d last time we visit HK...\n",
       "2       4  Thanks God it wasn   t too hot or too humid wh...\n",
       "3       4  HK Disneyland is a great compact park. Unfortu...\n",
       "4       4  the location is not in the city, took around 1..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('disney-dataset.csv')\n",
    "df_raw = df_raw.drop(['Review_ID', 'Year_Month', 'Reviewer_Location', 'Branch'], axis=1)\n",
    "df_raw = df_raw.rename(columns={\"Rating\": \"rating\", \"Review_Text\": \"review\"})\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuMTrTKPZNhE"
   },
   "source": [
    "### Tratamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5fq3QgadZQHE",
    "outputId": "d58e4804-b3c5-42d8-b14f-69a8697acc8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>ever disneyland anywhere  find disneyland hon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>since last time visit hk disneyland  yet  time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>thanks god hot humid visiting park otherwise w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>hk disneyland great compact park  unfortunatel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>location city  took around 1 hour kowlon  kids...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       4   ever disneyland anywhere  find disneyland hon...\n",
       "1       4  since last time visit hk disneyland  yet  time...\n",
       "2       4  thanks god hot humid visiting park otherwise w...\n",
       "3       4  hk disneyland great compact park  unfortunatel...\n",
       "4       4  location city  took around 1 hour kowlon  kids..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopword_and_punctuation_to_lower(df, column_name):\n",
    "\n",
    "  tokens_irrelevantes = nltk.corpus.stopwords.words(\"english\")\n",
    "  stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "  for ponto in punctuation:\n",
    "    tokens_irrelevantes.append(ponto)\n",
    "\n",
    "  token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "\n",
    "  frase_processada = list()\n",
    "  for review in df[column_name]:\n",
    "      nova_frase = list()\n",
    "      review = review.lower()\n",
    "      palavras_texto = token_pontuacao.tokenize(review)\n",
    "      for palavra in palavras_texto:\n",
    "          if palavra not in tokens_irrelevantes:\n",
    "              nova_frase.append(stemmer.stem(palavra))\n",
    "      frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "  return frase_processada\n",
    "\n",
    "\n",
    "\n",
    "def df_lower(df_column):\n",
    "    return df_column.apply(lambda x : x.lower())\n",
    "\n",
    "def remove_accents(df_column):\n",
    "    return [unidecode.unidecode(text) for text in df_column]\n",
    "\n",
    "def remove_punctuation(df_column):\n",
    "    \n",
    "    token_punct = tokenize.WordPunctTokenizer()\n",
    "    list_punct = list()\n",
    "    for punct in punctuation:\n",
    "        list_punct.append(punct)\n",
    "    \n",
    "    \n",
    "    list_phrases = list()\n",
    "    for text in df_column:\n",
    "        for punct in list_punct:\n",
    "            text = text.replace(punct, '')\n",
    "        list_phrases.append(text)\n",
    "\n",
    "    return list_phrases\n",
    "\n",
    "\n",
    "def remove_stopwords(df_column):\n",
    "    tokens_remove = nltk.corpus.stopwords.words(\"english\")\n",
    "    token_punct = tokenize.WordPunctTokenizer()\n",
    "    \n",
    "    list_phrases = list()\n",
    "    for text in df_column:\n",
    "        new_text = list()\n",
    "        words = token_punct.tokenize(text)\n",
    "        for word in words:\n",
    "            if word not in tokens_remove:\n",
    "                new_text.append(word)\n",
    "        list_phrases.append(' '.join(new_text))\n",
    "\n",
    "    return list_phrases\n",
    "\n",
    "\n",
    "def df_stem(df_column):\n",
    "\n",
    "    stemmer = nltk.RSLPStemmer()\n",
    "    token_punct = tokenize.WordPunctTokenizer()\n",
    "\n",
    "    list_phrases = list()\n",
    "    for text in df_column:\n",
    "        new_text = list()\n",
    "        words = token_punct.tokenize(text)\n",
    "        for word in words:\n",
    "            new_text.append(stemmer.stem(word))\n",
    "        list_phrases.append(' '.join(new_text))\n",
    "\n",
    "    return list_phrases\n",
    "\n",
    "df = df_raw.copy()\n",
    "#df.review = remove_stopword_and_punctuation_to_lower(df, 'review')\n",
    "df.review = df_lower(df_raw.review)\n",
    "df.review = remove_accents(df.review)\n",
    "df.review = remove_stopwords(df.review)\n",
    "df.review = remove_punctuation(df.review)\n",
    "#df.review = df_stem(df.review)\n",
    "\n",
    "x = df.review\n",
    "y = df.rating\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "yp_TRj38Lwuk"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PgvavBKQL8g"
   },
   "source": [
    "### Crie um modelo de Regressão Logística e imprima o r2 com os dados brutos (sem nenhum tratamento) do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "xcLifv5iSLDl",
    "outputId": "2940cb90-0064-4289-e0b6-f534b927f42e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(max_features=5000, strip_accents='unicode', min_df=10, ngram_range = (1,2))\n",
    "bw_train = tf.fit_transform(x_train)\n",
    "bw_test = tf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Fm1EmRulSsRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6275318829707427\n",
      "0.3718366802492108\n"
     ]
    }
   ],
   "source": [
    "regl = LogisticRegression(solver = \"lbfgs\", max_iter=150)\n",
    "regl.fit(bw_train, y_train)\n",
    "\n",
    "acc = regl.score(bw_test, y_test)\n",
    "\n",
    "y_pred = regl.predict(bw_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(acc)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
